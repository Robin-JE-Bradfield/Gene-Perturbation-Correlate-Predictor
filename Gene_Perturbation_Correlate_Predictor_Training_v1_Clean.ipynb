{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The code in this notebook is intended to use data from the Cell x Gene Census to train a model to predict how other genes might have changed given a perturbation of one or more other genes. It acquires the data (currently all naive B cell primary data), processes it to pair up cells and select a gene that differs in expression between them to perturb, and trains the model using this data.\n",
        "\n",
        "The model is an encoder-decoder transformer, which encodes the expression levels of genes in the source cell and then uses the encoded data as the keys and values to do cross-attention with queries derived from the perturbation. Genes are embedded using learnable embeddings, with the embedding then multiplied by the expression level using the ExpressionEmbedding class.\n",
        "\n",
        "The original inspiration for this was the idea of a model that could take an original cell state and a perturbation of one or more genes, and predict the consequences of that perturbation. This model is not capable of that, since the data it is provided with is not actual perturbation data and so it can only ever learn correlations, but given proper training data the architecture should be suitable.\n",
        "\n",
        "This is also still in development; at present, the loss fails to significantly decrease after five epochs. This is most likely due to poor data (naively selecting the first hundred protein-coding genes in the list leads to most having no expression or not being measured, judging by examples pulled from the dataloader, and pairs of cells selected for perturbation may have few measured genes in common), which could be resolved by better selection of genes and by an improved dataloader capable of caching cell examples until an appropriate pairing is found for them. However, it could also be due to the learning rate, or other factors."
      ],
      "metadata": {
        "id": "gaxpmjX1_PKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install_packages_&_import_libraries\n",
        "\n",
        "%pip install --quiet -U cellxgene_census\n",
        "%pip install --quiet tiledbsoma_ml\n",
        "\n",
        "import cellxgene_census\n",
        "import tiledbsoma as soma\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tiledbsoma_ml import ExperimentDataset\n",
        "from tiledbsoma_ml import experiment_dataloader\n",
        "import pandas as pd\n",
        "import anndata\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import os"
      ],
      "metadata": {
        "id": "BP_1fpwwsSJ8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get_census_data\n",
        "\n",
        "#See https://chanzuckerberg.github.io/cellxgene-census/notebooks/experimental/pytorch.html#Create-an-ExperimentDataPipe for guidance\n",
        "#on loading data.\n",
        "\n",
        "with cellxgene_census.open_soma(census_version = \"2025-01-30\") as census:\n",
        "  experiment = census[\"census_data\"][\"homo_sapiens\"]\n",
        "  experiment_rna = experiment.ms[\"RNA\"]\n",
        "  var_df = experiment_rna.var.read().concat().to_pandas()\n",
        "  gene_id_map = var_df.set_index(\"feature_id\")[\"soma_joinid\"].to_dict()\n",
        "  del var_df\n",
        "  datasets_df = census[\"census_info\"][\"datasets\"].read().concat().to_pandas()\n",
        "  datasets_id_map = datasets_df.set_index(\"dataset_id\")[\"soma_joinid\"].to_dict()\n",
        "  del datasets_df\n",
        "  presence_matrix = cellxgene_census.get_presence_matrix(census, \"homo_sapiens\")\n",
        "  #gene_id_map, datasets_id_map, and presence_matrix will be used later\n",
        "  #to mask out genes that weren't measured from attention, perturbation selection, and loss.\n",
        "  obs_filter = \"is_primary_data == True and cell_type == 'naive B cell'\"\n",
        "  var_filter = \"feature_type == 'protein_coding'\"\n",
        "\n",
        "  with experiment.axis_query(\n",
        "    measurement_name=\"RNA\",\n",
        "    obs_query=soma.AxisQuery(value_filter=obs_filter),\n",
        "    var_query=soma.AxisQuery(value_filter=var_filter)\n",
        "  ) as query:\n",
        "    #Set up experimental dataset.\n",
        "    experiment_dataset = ExperimentDataset(\n",
        "      query,\n",
        "      layer_name=\"raw\",\n",
        "      obs_column_names=[\"cell_type\", \"dataset_id\"],\n",
        "      batch_size=128,\n",
        "      shuffle=True,\n",
        "      seed=111,\n",
        "    )\n",
        "    obs_df = query.obs(column_names=[\"cell_type\", \"dataset_id\"]).concat().to_pandas()\n",
        "    var_df = query.var(column_names=[\"feature_id\", \"feature_length\"]).concat().to_pandas()\n",
        "    #Cell type is not currently used, since all the data originates from the same cell type,\n",
        "    #but in future the pairing function could be modified to select changes in expression from within a cell type,\n",
        "    #perhaps using a custom dataloader to cache examples drawn from the provided dataloader\n",
        "    #until a suitable pairing can be found for them.\n",
        "\n",
        "train_dataset, val_dataset = experiment_dataset.random_split(\n",
        "  0.8,\n",
        "  0.2,\n",
        "  seed=111,\n",
        ")\n",
        "\n",
        "train_dataloader = experiment_dataloader(train_dataset)\n",
        "val_dataloader = experiment_dataloader(val_dataset)\n",
        "\n",
        "#The above dataloaders produce entries with format (X, obs) where X is an array of counts\n",
        "#and obs is a dataframe containing cell type and dataset id.\n",
        "#Even filtering for protein-coding genes, however, there are ~20,000 genes, which is\n",
        "#too many given each gene attends to each other gene so the cost of self-attention\n",
        "#grows quadratically with gene count.\n",
        "\n",
        "#Currently, I am filtering for the first 100 genes, selected arbitrarily; future versions might\n",
        "#identify genes which are commonly measured and expressed and filter for those.\n",
        "\n",
        "valid_gene_indices = torch.arange(100) #(valid_gene_count)\n",
        "\n",
        "#Establish gene ID vocabulary (for selected genes only).\n",
        "gene_list = var_df.iloc[list(range(100)), 0].tolist()\n",
        "gene_vocab = {gene_ID:idx for idx, gene_ID in enumerate(gene_list)}\n",
        "#This vocabulary matches the Ensembl ID of each gene with its index *as provided in the filtered data*.\n",
        "#Contrast gene_id_map, which provides a similar feature for the index of a gene in the full Census,\n",
        "#allowing indexing into presence_matrix."
      ],
      "metadata": {
        "id": "dRxBSuyKAsAm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title data_processing_setup\n",
        "\n",
        "#First, we need to normalize input to derive TPM values rather than raw counts.\n",
        "#Then, we take log(TPM+1) to put the values on a more sensible scale.\n",
        "#Adding 1 sets the scale with 0 at the bottom,\n",
        "#since the raw counts are non-negative integers.\n",
        "\n",
        "def raw_to_log_TPM(counts_array, var_df, valid_genes = None):\n",
        "  \"Converts raw counts to log(TPM+1). If valid_genes is provided, also returns only TPMs for valid genes.\"\n",
        "  #counts_array should be a tensor (batch_size, gene_count)\n",
        "  #valid_genes should be a tensor (valid_gene_count) of indices of valid genes.\n",
        "  lengths = torch.tensor(var_df['feature_length']) / 1000 #(gene_count), converted to kilobases.\n",
        "  RPK = counts_array / lengths.unsqueeze(0) #(batch_size, gene_count), RPK = reads per kilobase.\n",
        "  total_counts = RPK.sum(dim=1) #(batch_size)\n",
        "  TPM = (RPK * 1e6) / total_counts.unsqueeze(1) #(batch_size, gene_count), TPM = transcripts per million.\n",
        "  if valid_genes is not None:\n",
        "    TPM = torch.index_select(TPM, dim=1, index=valid_genes.to(dtype = torch.long))\n",
        "  return torch.log(TPM + 1) #(batch_size, valid_gene_count)\n",
        "\n",
        "#Masking function, returns a boolean tensor mask with True on measured genes.\n",
        "\n",
        "def generate_presence_mask(batch, obs_batch, var_df, presence_matrix, gene_vocab, datasets_id_map, gene_id_map):\n",
        "  \"Returns a boolean mask for the provided batch with True on entries containing measured genes.\"\n",
        "  #batch should be a tensor of shape (batch_size, gene_count).\n",
        "  #batch must *not* be a pair_ or perturb_batch, or else the indexing will not work properly.\n",
        "  #obs_batch should be a pandas dataframe with a column named dataset_id.\n",
        "  #presence_matrix should be a sparse NP array of shape (num_datasets, total_num_genes).\n",
        "  #gene_vocab should be a dictionary of feature_id:index in the input tensor.\n",
        "  #The ID maps should be dictionaries from dataset/ensmebl id to index in the presence matrix.\n",
        "  inv_gene_vocab = {id:gene for gene, id in gene_vocab.items()}\n",
        "  dataset_ids = obs_batch['dataset_id'].to_list()\n",
        "  dataset_indices = [datasets_id_map[id] for id in dataset_ids]\n",
        "  gene_ensembl_ids = [inv_gene_vocab[id] for id in range(batch.shape[1])]\n",
        "  gene_indices = [gene_id_map[ensembl_id] for ensembl_id in gene_ensembl_ids]\n",
        "  mask = presence_matrix[np.array(dataset_indices)][:, np.array(gene_indices)].copy()\n",
        "  return torch.tensor(mask.toarray(), dtype = torch.bool)\n",
        "\n",
        "#We now need a function to take processed expression levels and generate the pairs\n",
        "#and perturbations that will actually be fed to the model.\n",
        "#For now we will *ignore cell type* and just take arbitrary pairs from within a batch;\n",
        "#current data is all from the cell type, so this is not a huge issue,\n",
        "#but an improved version could integrate a version of this function into a custom dataloader\n",
        "#and cache cells until a suitable match is found for them.\n",
        "\n",
        "def process_to_pairs(expression_batch, obs_batch, perturbed_gene_count):\n",
        "  \"\"\"Takes a batch, pairs up expression levels within it, and returns the pair, a perturbation between them, a presence mask\n",
        "  for the pair, and indices where no perturbation was possible.\"\"\"\n",
        "  #expression_batch should be a tensor (batch_size, valid_gene_count)\n",
        "  #perturbed genes is an integer, the gene to perturb will be selected randomly,\n",
        "  #from all genes that are measured in both pair members and have non-identical expression.\n",
        "  assert expression_batch.shape[0] % 2 == 0, \"Batch size must be divisible by 2\"\n",
        "  presence_mask = generate_presence_mask(expression_batch, obs_batch, var_df, presence_matrix, gene_vocab, datasets_id_map, gene_id_map)\n",
        "  pair_batch = expression_batch.unsqueeze(1).reshape(-1, 2, expression_batch.shape[-1]) #(batch_size/2, 2, valid_gene_count)\n",
        "  pair_presence_mask = presence_mask.unsqueeze(1).reshape(-1, 2, expression_batch.shape[-1]) #(batch_size/2, 2, valid_gene_count)\n",
        "  all_perturbs = pair_batch[:, 1, :] - pair_batch[:, 0, :] #(batch_size/2, valid_gene_couht)\n",
        "  #Get perturbation mask.\n",
        "  perturb_nonzero_mask = (all_perturbs != 0) #(batch_size/2, valid_gene_couht), identifies genes where there is nonzero difference in expression.\n",
        "  perturb_measured_mask = torch.logical_and(pair_presence_mask[:, 0, :], pair_presence_mask[:, 1, :]) #(batch_size/2, valid_gene_couht), identifies\n",
        "  #genes which were measured in both members of the pair.\n",
        "  perturb_double_mask = torch.logical_and(perturb_nonzero_mask, perturb_measured_mask)\n",
        "  perturb_rand_mask = torch.rand(all_perturbs.shape) * perturb_double_mask.to(torch.float)\n",
        "  _, perturb_topk = torch.topk(perturb_rand_mask, perturbed_gene_count, dim = 1)\n",
        "  #The above is a slightly hacky way of getting a random k genes from among the subset identified by the mask, but it works.\n",
        "  #Generate a mask that can be used to zero out all genes not selected for perturbation:\n",
        "  perturb_mask = torch.zeros(all_perturbs.shape)\n",
        "  perturb_mask[torch.arange(perturb_mask.shape[0]).unsqueeze(1).expand(-1, perturb_topk.shape[1]).to(torch.long), perturb_topk.to(torch.long)] = 1\n",
        "  #Detect rows with no valid perturbation targets:\n",
        "  no_perturb_indices = (torch.sum(perturb_double_mask.to(torch.float), dim = 1) == 0) #(batch_size/2)\n",
        "  #Apply mask and return outputs.\n",
        "  perturb_batch = (all_perturbs*perturb_mask).to(torch.float)\n",
        "  perturb_batch = torch.where(perturb_batch.abs() < 1e-6, 0.0, perturb_batch) #Clean up -0s to avoid possible weirdness down the line,\n",
        "  #I don't think it would actually cause any problems but it looks ugly.\n",
        "  return pair_batch, perturb_batch, pair_presence_mask, no_perturb_indices\n",
        "\n",
        "#Often, even if genes were measured, they have no expression in either cell, suggesting that they may be irrelevant\n",
        "#to the perturbation of interest. This takes up compute during attention for limited gain.\n",
        "#In practice this appears to have little effect on performance, but it may increase noise during training, so I've\n",
        "#included masking for genes with no expression in either pair member.\n",
        "#Importantly, this mask will not be used for the loss since we still want the network to predict results for these genes.\n",
        "#It also is only applied to the encoder portion, again since all gene should be predicted.\n",
        "\n",
        "def generate_pair_mask(pair_batch):\n",
        "  #pair_batch should be a tensor of shape (batch_size/2, 2, valid_gene_count).\n",
        "  return torch.logical_and(pair_batch[:, 0, :] == 0, pair_batch[:, 1, :] == 0).unsqueeze(1).expand(-1, 2, -1)\n",
        "\n",
        "#Now let's wrap all these up into a single function that we can call easily.\n",
        "\n",
        "def process_batch(X, obs_batch, perturbation_count):\n",
        "  #X should have shape (batch_size, valid_gene_count) and be a Numpy array.\n",
        "  X = torch.tensor(X, dtype = torch.float)\n",
        "  X = raw_to_log_TPM(X, var_df, valid_gene_indices)\n",
        "  pairs, perturbs, pair_presence_mask, no_perturb_indices = process_to_pairs(X, obs_batch, perturbation_count)\n",
        "  #Generate a mask for genes that aren't expressed in either member of a pair.\n",
        "  pair_zeros_mask = generate_pair_mask(pairs) #(batch_size/2, valid_gene_count), True on double-zero genes.\n",
        "  #Create a combined mask for source non-measured or doubly non-expressed genes.\n",
        "  #This will be used for attention masking, but *not* for loss\n",
        "  #since we still want the network to predict non-expressed genes.\n",
        "  pair_mask = torch.logical_and((pair_presence_mask == False), pair_zeros_mask) #(batch_size/2, 2, valid_gene_count), True on genes that should be masked.\n",
        "  #Create a combined mask to block out pairs where no perturbation was possible.\n",
        "  #This will be used for the loss, but *not* for attention masking\n",
        "  #since an attention mask with all inputs masked yields nan.\n",
        "  combined_mask = torch.logical_and(pair_presence_mask, (no_perturb_indices == False).unsqueeze(-1).unsqueeze(-1).expand(-1, 2, pair_presence_mask.shape[2]))\n",
        "  #Return the pair members, the perturbation, the pair presence mask for attention, and the combined mask for use with loss.\n",
        "  return pairs[:,0,:], pairs[:,1,:], perturbs, pair_mask, combined_mask #first three (batch_size/2, valid_gene_count), fourth & fifth (batch_size/2, 2, valid_gene_count)"
      ],
      "metadata": {
        "id": "v9YYLDehr3o5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title model_setup\n",
        "\n",
        "#Convenience functions.\n",
        "def cpu():\n",
        "  return torch.device('cpu')\n",
        "\n",
        "def gpu(i=0):\n",
        "  return torch.device(f'cuda:{i}')\n",
        "\n",
        "def num_gpus():\n",
        "  return torch.cuda.device_count()\n",
        "\n",
        "def try_gpu(i=0):\n",
        "  if num_gpus() >= i+1:\n",
        "    return gpu(i)\n",
        "  return cpu()\n",
        "\n",
        "#Custom embedding for gene expression levels.\n",
        "class ExpressionEmbedding(nn.Module):\n",
        "  'Embeds numeric gene IDs and multiplies the embedded vectors by the provided expression levels.'\n",
        "  def __init__(self, gene_count, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(gene_count, embedding_dim)\n",
        "\n",
        "  def forward(self, X, genes):\n",
        "    #X should be a tensor of expression levels of shape (batch_size, gene_count).\n",
        "    #genes should be a tensor of numeric tokens of shape (gene_count).\n",
        "    embedded_genes = self.embed(genes) #(gene_count, n_hiddens)\n",
        "    embedded_counts = X.unsqueeze(-1) * embedded_genes.unsqueeze(0) #(batch_size, gene_count, n_hiddens)\n",
        "    return embedded_counts\n",
        "\n",
        "#Convenience class, derived from the D2L Dive Into Deep Learning textbook with some modifications.\n",
        "class AddNorm(nn.Module):\n",
        "  def __init__(self, norm_shape, dropout = 0, use_dropout = False):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout) if use_dropout else nn.Identity()\n",
        "    self.ln = nn.LayerNorm(norm_shape)\n",
        "\n",
        "  def forward(self, X, Y):\n",
        "    return self.ln(self.dropout(Y) + X)\n",
        "\n",
        "#Building block of the encoder. Uses the pair mask to mask out genes from attention.\n",
        "class TransformerGeneEncoderBlock(nn.Module):\n",
        "  def __init__(self, n_hiddens, ff_n_hiddens, n_heads, batch_first, dropout, norm_dropout = False):\n",
        "    super().__init__()\n",
        "    self.attention = nn.MultiheadAttention(n_hiddens, n_heads, batch_first=batch_first, dropout=dropout)\n",
        "    self.addnorm1 = AddNorm(n_hiddens, dropout, use_dropout = norm_dropout)\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(n_hiddens, ff_n_hiddens),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_n_hiddens, n_hiddens)\n",
        "    )\n",
        "    self.addnorm2 = AddNorm(n_hiddens, dropout, use_dropout = norm_dropout)\n",
        "\n",
        "  def forward(self, X, pair_mask = None):\n",
        "    Y, _ = self.attention(X, X ,X, key_padding_mask = pair_mask)\n",
        "    Y = self.addnorm1(X, Y)\n",
        "    return self.addnorm2(Y, self.ffn(Y))\n",
        "\n",
        "#The complete encoder. Note that positional encoding is not used,\n",
        "#since no information should be provided by the order of the genes.\n",
        "class TransformerGeneEncoder(nn.Module):\n",
        "  def __init__(self, vocab_size, n_hiddens, ff_n_hiddens, n_heads, n_blks, dropout, norm_dropout = False, batch_first = True):\n",
        "    super().__init__()\n",
        "    self.embed = ExpressionEmbedding(vocab_size, n_hiddens)\n",
        "    self.n_hiddens = n_hiddens\n",
        "    self.blks = nn.Sequential()\n",
        "    for i in range(n_blks):\n",
        "      self.blks.add_module(\"block\"+str(i), TransformerGeneEncoderBlock(n_hiddens, ff_n_hiddens, n_heads, batch_first, dropout, norm_dropout))\n",
        "\n",
        "  def forward(self, X, genes, pair_mask = None):\n",
        "    X = self.embed(X, genes) * math.sqrt(self.n_hiddens)\n",
        "    for i, blk in enumerate(self.blks):\n",
        "      X = blk(X, pair_mask)\n",
        "    return X\n",
        "\n",
        "#The building block of the decoder.\n",
        "class TransformerGeneDecoderBlock(nn.Module):\n",
        "  def __init__(self, n_hiddens, ff_n_hiddens, n_heads, batch_first, dropout, i, norm_dropout = False, use_cross_attention = True):\n",
        "    super().__init__()\n",
        "    self.i = i\n",
        "    self.attention1 = nn.MultiheadAttention(n_hiddens, n_heads, batch_first=batch_first, dropout=dropout)\n",
        "    self.addnorm1 = AddNorm(n_hiddens, dropout, use_dropout = norm_dropout)\n",
        "    self.attention2 = nn.MultiheadAttention(n_hiddens, n_heads, batch_first=batch_first, dropout=dropout) if use_cross_attention else None\n",
        "    self.addnorm2 = AddNorm(n_hiddens, dropout, use_dropout = norm_dropout) if use_cross_attention else None\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(n_hiddens, ff_n_hiddens),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_n_hiddens, n_hiddens)\n",
        "    )\n",
        "    self.addnorm3 = AddNorm(n_hiddens, dropout, use_dropout = norm_dropout)\n",
        "    self.n_heads = n_heads\n",
        "    self.use_cross_attention = use_cross_attention\n",
        "\n",
        "  def forward(self, X, state):\n",
        "    #State has structure [encoder_outputs, encoder_pair_mask, decoded_outputs].\n",
        "    #Some of this is actually redundant - this isn't a sequence model so there's no need to store decoded outputs,\n",
        "    #but I'm leaving the structure unchanged for now in case I find a use for it later.\n",
        "    enc_outputs, enc_pair_mask = state[0], state[1]\n",
        "    if state[2][self.i] is None:\n",
        "      key_values = X\n",
        "    else:\n",
        "      key_values = torch.cat((state[2][self.i], X), dim = 1)\n",
        "    state[2][self.i] = key_values\n",
        "\n",
        "    X2, _ = self.attention1(X, key_values, key_values)\n",
        "    Y = self.addnorm1(X, X2)\n",
        "    if self.use_cross_attention:\n",
        "      Y2, _ = self.attention2(Y, enc_outputs, enc_outputs, key_padding_mask = enc_pair_mask)\n",
        "      Z = self.addnorm2(Y, Y2)\n",
        "      return self.addnorm3(Z, self.ffn(Z)), state\n",
        "    else:\n",
        "      return self.addnorm3(Y, self.ffn(Y)), state\n",
        "\n",
        "#The full decoder. As with the encoder, no positional encoding is used.\n",
        "class TransformerGeneDecoder(nn.Module):\n",
        "  def __init__(self, vocab_size, n_hiddens, ff_n_hiddens, n_heads, n_blks, dropout, norm_dropout = False, batch_first = True, use_cross_attention = True):\n",
        "    super().__init__()\n",
        "    self.embed = ExpressionEmbedding(vocab_size, n_hiddens)\n",
        "    self.n_hiddens = n_hiddens\n",
        "    self.n_blks = n_blks\n",
        "    self.blks = nn.Sequential()\n",
        "    for i in range(n_blks):\n",
        "      self.blks.add_module(\"block\"+str(i), TransformerGeneDecoderBlock(n_hiddens, ff_n_hiddens, n_heads, batch_first, dropout, i, norm_dropout, use_cross_attention))\n",
        "\n",
        "  def init_state(self, enc_outputs, enc_pair_mask = None):\n",
        "    return [enc_outputs, enc_pair_mask, [None]*self.n_blks]\n",
        "\n",
        "  def forward(self, X, genes, state):\n",
        "    X = self.embed(X, genes) * math.sqrt(self.n_hiddens)\n",
        "    for i, blk in enumerate(self.blks):\n",
        "      X, state = blk(X, state)\n",
        "    return X, state\n",
        "\n",
        "#The full model. The reduction of representations to a single value might well be an issue for overall performance; however,\n",
        "#it's serviceable for now.\n",
        "class GenePerturbationTransformer(nn.Module):\n",
        "  def __init__(self, vocab_size, n_hiddens, ff_n_hiddens, n_enc_heads, n_enc_blks, n_dec_heads, n_dec_blks, dropout, norm_dropout = False):\n",
        "    super().__init__()\n",
        "    self.encoder = TransformerGeneEncoder(vocab_size, n_hiddens, ff_n_hiddens, n_enc_heads, n_enc_blks, dropout, norm_dropout)\n",
        "    self.decoder = TransformerGeneDecoder(vocab_size, n_hiddens, ff_n_hiddens, n_dec_heads, n_dec_blks, dropout, norm_dropout, use_cross_attention=True)\n",
        "    self.out = nn.Linear(n_hiddens, 1)\n",
        "\n",
        "  def forward(self, X, P, genes, pair_mask = None):\n",
        "    #X is source expressions, P is perturbations.\n",
        "    X = self.encoder(X, genes)\n",
        "    state = self.decoder.init_state(X, pair_mask)\n",
        "    Z, _ = self.decoder(P, genes, state)\n",
        "    return self.out(Z).squeeze(-1) #Shape (batch_size, gene_count), batch_size /2 from original and only valid genes."
      ],
      "metadata": {
        "id": "I1y8TChhAZvT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title training_loop\n",
        "\n",
        "#A notable potential improvement here is that based on testing with next(iter()), initializing an instance of the dataloader for the data used in this version\n",
        "#takes up to 10 minutes. This means it might be possible to set it up to happen asynchronously on the CPU while the GPU is busy training the model\n",
        "#and the CPU's load is relatively light, potentially saving significant amounts of time.\n",
        "\n",
        "perturbed_gene_count = 1\n",
        "\n",
        "n_hiddens = 128\n",
        "ff_n_hiddens = 512\n",
        "n_enc_heads = 4\n",
        "n_enc_blks = 4\n",
        "n_dec_heads = 4\n",
        "n_dec_blks = 8\n",
        "dropout = 0.2\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "max_epochs = 5\n",
        "device = try_gpu()\n",
        "\n",
        "genes = torch.tensor([gene_vocab[gene_ID] for gene_ID in gene_list])\n",
        "model = GenePerturbationTransformer(genes.shape[0], n_hiddens, ff_n_hiddens, n_enc_heads, n_enc_blks, n_dec_heads, n_dec_blks, dropout).to(device)\n",
        "genes = genes.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fn = torch.nn.MSELoss() #This may not be the best loss function, but it's fine for now.\n",
        "\n",
        "#For loss tracking:\n",
        "train_length = len(train_dataloader)\n",
        "val_length = len(val_dataloader)\n",
        "\n",
        "print(\"Beginning training.\")\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    total_train_loss = torch.tensor(0.0, device = device)\n",
        "    total_val_loss = torch.tensor(0.0, device = device)\n",
        "    time_initial = time.time()\n",
        "    model.train()\n",
        "    print(f\"Epoch {epoch}: Training ... \\t Time:{time.time()-time_initial}\")\n",
        "    for X, obs in train_dataloader:\n",
        "        if X.shape[0] != 128: #Handles last batch being smaller, since drop_last isn't available from tiledbsoma_ml's experiment dataloader.\n",
        "          continue            #Fixing this would be yet another benefit of a custom dataloader!\n",
        "        X, Z, P, pair_mask, combined_mask = process_batch(X, obs, perturbed_gene_count)\n",
        "        X, Z, P, pair_mask, combined_mask = X.to(device), Z.to(device), P.to(device), pair_mask.to(device), combined_mask.to(device)\n",
        "        source_mask = pair_mask[:, 0, :] #Restricts attention to genes that were measured in the source cell.\n",
        "        Y = model(X, P, genes, source_mask)\n",
        "        #Mask out genes that weren't measured in the target and batches where no perturbation occurred.\n",
        "        #Remember combined_mask has True where genes should not be blocked.\n",
        "        loss_mask = combined_mask[:, 1, :].to(torch.float)\n",
        "        loss_scale_factor = loss_mask.numel() / torch.sum(loss_mask) #Rescale to account for ignoring some elements,\n",
        "        #will cause problems if loss_mask is entirely zeros but this is unlikely to happen. I may fix it regardless in\n",
        "        #a future version.\n",
        "        Y, Z = Y * loss_mask, Z * loss_mask\n",
        "        loss = loss_fn(Y, Z) * loss_scale_factor\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.detach()\n",
        "    model.eval()\n",
        "    print(f\"Epoch {epoch}: Validating... \\t Time:{time.time()-time_initial}\")\n",
        "    for X, obs in val_dataloader:\n",
        "        if X.shape[0] != 128:\n",
        "          continue\n",
        "        X, Z, P, pair_mask, combined_mask = process_batch(X, obs, perturbed_gene_count)\n",
        "        X, Z, P, pair_mask, combined_mask = X.to(device), Z.to(device), P.to(device), pair_mask.to(device), combined_mask.to(device)\n",
        "        source_mask = pair_mask[:, 0, :]\n",
        "        with torch.no_grad():\n",
        "          Y = model(X, P, genes, source_mask)\n",
        "          loss_mask = combined_mask[:, 1, :].to(torch.float)\n",
        "          loss_scale_factor = loss_mask.numel() / torch.sum(loss_mask)\n",
        "          Y, Z = Y * loss_mask, Z * loss_mask\n",
        "          loss = loss_fn(Y, Z) * loss_scale_factor\n",
        "          total_val_loss += loss.detach()\n",
        "    avg_train_loss = total_train_loss.item()/train_length\n",
        "    avg_val_loss = total_val_loss.item()/val_length\n",
        "    time_elapsed = time.time() - time_initial\n",
        "    print(f\"Epoch {epoch}: Training Loss {avg_train_loss:.4f}, Validation Loss {avg_val_loss:.4f}, Time Elapsed {time_elapsed}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "gq3ls3nWT8xj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download_weights\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "model.to(cpu())\n",
        "filename = 'gene_perturbation_correlate_predictor_weights_v1_5_epochs.txt'\n",
        "torch.save(model.state_dict(), filename)\n",
        "files.download(filename)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XjOEg8ZL-7HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download_vocab\n",
        "\n",
        "import json\n",
        "\n",
        "model_key = 'GPCP_v1_e5'\n",
        "\n",
        "with open(f'gene_vocab_{model_key}.json', 'w') as f:\n",
        "  json.dump(gene_vocab, f)\n",
        "\n",
        "files.download(f'gene_vocab_{model_key}.json')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3KZncJmzC3l_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}